{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Which command method looks through a tag’s descendants and retrieves all descendants that match your filters?\n",
    "\tfind_all()\n",
    "\n",
    "2. Which operating system among the following is not supported by selenium.\n",
    "\tSolaris\n",
    "\n",
    "3. Which of these languages is NOT supported by the Selenium RC.\n",
    "\tASP\n",
    "\n",
    "4. Which of these commands is used to enter text in text_boxes.\n",
    "\tsend_keys()\n",
    "\n",
    "5. Websites fetched by crawler are indexed and kept in huge database, this process is called as:\n",
    "\tIndexing\n",
    "\n",
    "6. Web Crawler is also called as:\n",
    "\tWeb-spider\n",
    "\n",
    "7. Which selenium command among the following makes WebDriver wait for a certain condition to occur before proceeding further with execution.\n",
    "\tWebDriverWait\n",
    "\n",
    "8. Which of the following are commonly used expected conditions in Selenium?\n",
    "\ttitle_contains\n",
    "\n",
    "9. Where xpath is used?\n",
    "\tXML\n",
    "\n",
    "10. Which of the following commands is used to find an element in a webpage from the id attribute of its tag.\n",
    "\tfind_element_by_id\n",
    "\n",
    "11. Consider the following line in a HTML code.\n",
    "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"># Lacie # </a>\n",
    "Which of the following are attributes of <a> tag here?\n",
    "\tclass\n",
    "\thref\n",
    "\n",
    "12. Which among the following is/are parsers used in BeautifulSoup?\n",
    "\thref\n",
    "\tlxml\n",
    "\n",
    "13. Select the packages you require generally in web-scraping.\n",
    "\tSelenium\n",
    "\tBeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below questions have been solved in \"WEB SCRAPING – WORKSHEET 2 (My Answers).ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write a python program to scrap 10 images of Rayban Sunglasses from flipkart website and save them in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os, shutil, requests\n",
    "\n",
    "# Run webdriver for Chrome\n",
    "driver = webdriver.Chrome('chromedriver.exe') # This will open an automated Chrome Browser window. DON'T close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the website to scrap. This will load the website in the automated window. DON'T manually change the website on the browser URL.\n",
    "driver.get('https://www.flipkart.com/sunglasses/pr?p%5B%5D=facets.ideal_for%255B%255D%3DMen&sid=26x&otracker=nmenu_sub_Men_0_Sunglasses&otracker=nmenu_sub_Men_0_Sunglasses&p%5B%5D=facets.brand%255B%255D%3DRay-Ban')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Directory\n",
    "def make_directory(dirname):\n",
    "    current_path = os.getcwd()\n",
    "    path = os.path.join(current_path, dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'RayBan'\n",
    "make_directory(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images captured :: 10\n"
     ]
    }
   ],
   "source": [
    "images = driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")[0:10]\n",
    "print('Number of images captured ::', len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Extract URLs\n",
    "urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    urls.append(source)\n",
    "    print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for index, link in enumerate(urls):\n",
    "    print(\"Downloading {0} of {1} images\".format(index+1, len(urls)))\n",
    "    response = requests.get(link)\n",
    "    with open('{0}/img_{1}.jpeg'.format(dir_name, index), \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Write a program to scrap 20 mouses’ data which includes model name, price from www.amazon.in whose price is less than 500 and make a data frame with 2 columns “model_name”,” price” with the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Run webdriver for Chrome\n",
    "driver = webdriver.Chrome('chromedriver.exe') # This will open an automated Chrome Browser window. DON'T close it.\n",
    "\n",
    "# Load the website to scrap. This will load the website in the automated window. DON'T manually change the website on the browser URL.\n",
    "driver.get('https://www.amazon.in/s?k=mouse&i=computers&rh=n%3A976392031%2Cp_89%3AAmazonBasics%7CDell%7CHP%7CLenovo%7CLogitech%2Cp_36%3A-50000&dc&qid=1601122143&rnid=1318502031&ref=sr_nr_p_36_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")[0:10]\n",
    "prices = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = {}\n",
    "product_data['model_name'] = []\n",
    "product_data['price'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in title:\n",
    "    product_data['model_name'].append((name.text).split()[0]) # .text will help us to extract values between the tags\n",
    "print(product_data['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for price in prices:\n",
    "    product_data['price'].append(price.text) # .text will help us to extract values between the tags\n",
    "print(product_data['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in csv\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(product_data)\n",
    "print(df)\n",
    "\n",
    "# df.to_csv('product_data.csv', mode='a', encoding='utf-8-sig') # mode='a' is to open the file in append mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
