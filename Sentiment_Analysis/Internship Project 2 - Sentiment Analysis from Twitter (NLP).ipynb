{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, start, end):\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query) \\\n",
    "        .setSince(start) \\\n",
    "        .setUntil(end) \\\n",
    "        .setMaxTweets(200)\n",
    "    # Create list of tweets\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    # Extract text from tweets\n",
    "    text_tweets = [[tweet.text] for tweet in tweets]\n",
    "    return text_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = \" \".join(str(text[word][0]) for word in range(0, len(text)))\n",
    "    text_lower = text.lower()\n",
    "    cleaned_text = text_lower.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokenized_words = word_tokenize(cleaned_text, \"english\")\n",
    "    print(\"Total Number of Words ::\", len(tokenized_words))\n",
    "\n",
    "    final_words, strings, lemma_words = [], [], []\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            final_words.append(word)\n",
    "    print(\"After filtering stopwords ::\", len(final_words))\n",
    "\n",
    "    for word in final_words:\n",
    "        word = \"\".join(re.split(\"[^a-zA-Z]\", word))\n",
    "        strings.append(word)\n",
    "    print(\"Without characters/symbols ::\", len(strings))\n",
    "\n",
    "    for word in strings:\n",
    "        word = WordNetLemmatizer().lemmatize(word)\n",
    "        lemma_words.append(word)\n",
    "    print(\"Lemmatized Words ::\", len(lemma_words))\n",
    "\n",
    "#     df = pd.DataFrame([final_words, strings, lemma_words], index=['final_words', 'strings', 'lemma_words']).T\n",
    "#     df.to_csv('coronavirus_words.csv', index=False)\n",
    "\n",
    "    lemma_final = \" \".join(str(word) for word in lemma_words)\n",
    "    print(\">>>> Processing Text Complete\")\n",
    "    return lemma_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(string):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sent = sia.polarity_scores(string)\n",
    "    print(\"\\n\", sent)\n",
    "    if (sent['neg']>sent['pos']) & (sent['neu']>sent['neg']):\n",
    "        print(\">> Mostly Neutral with Negative Sentiment\")\n",
    "    elif (sent['pos']>sent['neg']) & (sent['neu']>sent['pos']):\n",
    "        print(\">> Mostly Neutral with Positive Sentiment\")\n",
    "    elif (sent['neg']>sent['pos']) & (sent['neu']<sent['neg']):\n",
    "        print(\">> Negative Sentiment\")\n",
    "    else: print(\">> Positive Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(search, start_date, end_date):\n",
    "    text_tweets = get_tweets(search, start_date, end_date)\n",
    "    processed_text = process_text(text_tweets)\n",
    "    sentiment(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words :: 3457\n",
      "After filtering stopwords :: 2961\n",
      "Without characters/symbols :: 2961\n",
      "Lemmatized Words :: 2961\n",
      ">>>> Processing Text Complete\n",
      "\n",
      " {'neg': 0.044, 'neu': 0.904, 'pos': 0.052, 'compound': 0.9257}\n",
      ">> Mostly Neutral with Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "main('coronavirus', '2019-12-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words :: 4413\n",
      "After filtering stopwords :: 3510\n",
      "Without characters/symbols :: 3510\n",
      "Lemmatized Words :: 3510\n",
      ">>>> Processing Text Complete\n",
      "\n",
      " {'neg': 0.085, 'neu': 0.851, 'pos': 0.064, 'compound': -0.9978}\n",
      ">> Mostly Neutral with Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "main('coronavirus', '2020-04-01', '2020-04-30')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
