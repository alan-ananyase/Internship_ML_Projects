{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "- Word vectorization is the process of mapping words to a set of real numbers or vectors. This is done to process the given words using machine learning techniques and extract relevant information from them such that it can be used in further predicting words. Vectorization is done by comparing a given word to the corpus(collection) of the available words. \n",
    "\n",
    "- It is language modeling and feature learning technique. Word embedding is a way to perform mapping using a neural network. \n",
    "- There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook).\n",
    "- We are going to discuss about word2vec in this tutorial\n",
    "\n",
    "## Where it is being used\n",
    "- `Compute similar words: `Word embedding is used to suggest similar words to the word being subjected to the prediction model. Along with that it also suggests dissimilar words, as well as most common words.\n",
    "\n",
    "- `Create a group of related words:` It is used for semantic grouping which will group things of similar characteristic together and dissimilar far away.\n",
    "\n",
    "- `Feature for text classification: `Text is mapped into arrays of vectors which is fed to the model for training as well as prediction. Text-based classifier models cannot be trained on the string, so this will convert the text into machine trainable form. Further its features of building semantic help in text-based classification.\n",
    "\n",
    "- `Document clustering` is another application where word embedding is widely used\n",
    "\n",
    "- `Natural language processing:` There are many applications where word embedding is useful and wins over feature extraction phases such as parts of speech tagging, sentimental analysis, and syntactic analysis.\n",
    "Now we have got some knowledge of word embedding. Some light is also thrown on different models to implement word embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "Count vectorizer uses two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
    "\n",
    "#### Bag of Words Model\n",
    "BOW model is used in NLP to represent the given text/sentence/document as a collection (bag) of words without giving any importance to grammar or the occurrence order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "BOW1 = {I :2, went : 1, to : 1,have : 1, a : 1, cup: 1, of :1, coffee : 1, but :1, ended : 1, up :1,having : 1, with :1, her :1}\n",
    "\n",
    "BOW2 = {I : 1, don’t : 1, understand:1, what : 1 , is :1, the : 1, problem : 1, here : 1}\n",
    "\n",
    "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occurring terms. But the “stop words” are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn’t mean that the word is as important. To resolve this problem, “Tf-idf” was introduced. We will discuss about it later.\n",
    "\n",
    "#### n-gram model\n",
    "\n",
    "As discussed in bag of words model, BOW model doesn’t keep the sequence of words in a given text, only the frequency of words matters. It doesn’t take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text.n-gram model is used in such cases to keep the context of the given text intact. N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "When, n= 1, we call it a “unigram”.\n",
    "\n",
    "             n=2, it is called a “bigram”. \n",
    "             \n",
    "             n=3, it is called a “trigram”.\n",
    "And so on.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "* Unigram \n",
    "\n",
    "[I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
    "\n",
    "* Bi-gram\n",
    "\n",
    "[I went], [went to],[to have],[have a],[a cup],[cup f],[of coffee],[coffee but],[but I],[I ended],[ended up],\n",
    "[up having],[having lunch],[lunch with],[with her]\n",
    "\n",
    "* Tri-gram\n",
    "\n",
    "[I went to], [went to have], [to have a], [have a cup],[ a cup of], [cup of coffee],[ of coffee but],[ coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
    "\n",
    "Note: We can clearly see that BOW model is nothing but n-gram model when n=1.\n",
    "\n",
    "Skip-grams\n",
    "\n",
    "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. \n",
    "Example:\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
    "\n",
    "[I understand, don’t what, understand is, what the, is problem, the here].\n",
    "\n",
    "\n",
    "Let's see the implementation of Count vectorizer in python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Example of single document\n",
    "# Without stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Single document (',' seperates each document)\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "\n",
    "# This step will convert text into tokens \n",
    "vect1 = CountVectorizer()\n",
    "\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit and transform and predict if the word is present or not\n",
    "- This is widely used for document or subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c_vect = CountVectorizer()\n",
    "\n",
    "c_vect.fit(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Present at  [[0 1 0 0 0 0 0]]\n",
      "original indexes ['bag', 'example', 'words']\n"
     ]
    }
   ],
   "source": [
    "string2 = ['Lets understand  bag']\n",
    "\n",
    "c_new_vect = c_vect.transform(string2)\n",
    "\n",
    "print (\"Text Present at \",c_new_vect.toarray())\n",
    "\n",
    "# Compare with the indexes\n",
    "print (\"original indexes\", vect1.get_feature_names() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None,\n",
      "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...],\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words using stopwords (you can avoid writing extra steps to remove stopwords)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "vect1 = CountVectorizer(stop_words=stop_words)\n",
    "print (vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['bag', 'example', 'words']\n",
      "vocab        : {'example': 1, 'bag': 0, 'words': 2}\n"
     ]
    }
   ],
   "source": [
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\",vect1.get_feature_names())\n",
    "print(\"vocab        :\",vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using function\n",
    "def text_matrix(message, countvect):\n",
    "    terms_doc = countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below metrix is the Bag of Words approach\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>but</th>\n",
       "      <th>for</th>\n",
       "      <th>get</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>making</th>\n",
       "      <th>mantra</th>\n",
       "      <th>natural</th>\n",
       "      <th>only</th>\n",
       "      <th>practice</th>\n",
       "      <th>processing</th>\n",
       "      <th>progress</th>\n",
       "      <th>slowly</th>\n",
       "      <th>success</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  but  for  get  in  is  language  making  mantra  natural  only  \\\n",
       "0    1    0    0    0   1   0         1       1       0        1     0   \n",
       "1    0    0    0    1   0   0         0       0       0        0     0   \n",
       "2    0    1    1    0   0   1         0       0       1        0     1   \n",
       "\n",
       "   practice  processing  progress  slowly  success  the  there  we  will  \n",
       "0         0           1         1       1        0    0      0   1     0  \n",
       "1         0           0         0       0        0    0      1   1     1  \n",
       "2         1           0         0       0        1    1      0   0     0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = ['We are slowly making progress in Natural Language Processing',\n",
    "          \"We will get there\", \"But practice is the only mantra for success\" ]\n",
    "\n",
    "c_vect = CountVectorizer()\n",
    "print (\"Below metrix is the Bag of Words approach\")\n",
    "text_matrix(message, c_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram  : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram  : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram  : ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = [\"This is an example of n-gram!\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "\n",
    "vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "print(\"1-gram  :\",vect1.get_feature_names())\n",
    "\n",
    "print(\"2-gram  :\",vect2.get_feature_names())\n",
    "print(\"3-gram  :\",vect3.get_feature_names())\n",
    "print(\"4-gram  :\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf (Term frequency–Inverse document frequency)\n",
    "\n",
    "Wikipedia definition:  ” Tf-Idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The Tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.”\n",
    "\n",
    "\n",
    "### Term Frequency\n",
    "It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as:\n",
    "\n",
    "Term frequency = (Number of times a word appears in the document) / (Total number of words in the document)\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "Term frequency has a disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like ‘a’, ‘the’, ‘in’, ’of’ etc. appears more in the documents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less.\n",
    " To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurrence. Mathematically it is given as:\n",
    " \n",
    "Inverse Document Frequency = log [(Number of documents)/(Number of documents the word appears in)]   \n",
    "\n",
    "**note: [log has base 2]**\n",
    "\n",
    "\n",
    "*Tf-Idf Score = Term frequency * Inverse Document Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.584962500721156"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.log2(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand more with an example:\n",
    "\n",
    "Doc 1: This is an example.\n",
    "\n",
    "Doc 2: We will see how it works.\n",
    "\n",
    "Doc 3: IDF can be confusing.\n",
    "\n",
    "\n",
    "\n",
    "<img src= \"tfidf.PNG\">\n",
    "\n",
    "In the above table, we have calculated the term frequency as well as inverse document frequency of each of the words present in the 3 documents given. \n",
    "\n",
    "Now, let's calculate the tf-idf score for each term. Since, words of one document is not present in another document, we will have tf-idf value 0 for them e.g. words of doc1 will have 0 tf-idf for doc2 and doc3.\n",
    "\n",
    "<img src= \"tfidf2.PNG\">\n",
    "\n",
    "Great, hope this example must have cleared how Tf-Idf works. \n",
    "\n",
    "let's see the python implementation for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
       "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
       "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
       "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
       "\n",
       "   this        we      will     works  \n",
       "0   0.5  0.000000  0.000000  0.000000  \n",
       "1   0.0  0.408248  0.408248  0.408248  \n",
       "2   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc= [\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "#print(tfid.get_feature_names())\n",
    "\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df\n",
    "#print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are using the same data set as we used while doing manual calculation, the results are different than what we got.\n",
    "\n",
    "This is because sklearn package have some modifications done to the formula to avoid complete avoidance of terms as well as to counter dividing by zero. \n",
    "\n",
    "You can know more by going through the official doumentation of sklearn as below:\n",
    "\n",
    "\"\n",
    "   *The formula that is used to compute the tf-idf for a term t of a document d\n",
    "    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
    "    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
    "    n is the total number of documents in the document set and df(t) is the\n",
    "    document frequency of t; the document frequency is the number of documents\n",
    "    in the document set that contain the term t. The effect of adding \"1\" to\n",
    "    the idf in the equation above is that terms with zero idf, i.e., terms\n",
    "    that occur in all documents in a training set, will not be entirely\n",
    "    ignored.\n",
    "    (Note that the idf formula above differs from the standard textbook\n",
    "    notation that defines the idf as\n",
    "    idf(t) = log [ n / (df(t) + 1) ]).\n",
    "    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
    "    numerator and denominator of the idf as if an extra document was seen\n",
    "    containing every term in the collection exactly once, which prevents\n",
    "    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covid</th>\n",
       "      <th>is</th>\n",
       "      <th>nothing</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.409937</td>\n",
       "      <td>0.409937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576152</td>\n",
       "      <td>0.576152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      covid        is   nothing      that      what\n",
       "0  0.409937  0.409937  0.000000  0.576152  0.576152\n",
       "1  0.501549  0.501549  0.704909  0.000000  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will call the function created earlier\n",
    "feb_message = [\"What is that covid\",\n",
    "              \"covid is nothing\"]\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "#Passing same message with TF-IDF\n",
    "\n",
    "text_matrix(feb_message,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>covid</th>\n",
       "      <th>is</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668501</td>\n",
       "      <td>0.334251</td>\n",
       "      <td>0.469778</td>\n",
       "      <td>0.469778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bad     covid        is      that      what\n",
       "0  0.000000  0.668501  0.334251  0.469778  0.469778\n",
       "1  0.704909  0.501549  0.501549  0.000000  0.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importance of Covid increased based on the occurance and total document\n",
    "jul_message = [\"What is that covid covid\",\n",
    "              \"covid is bad\"]\n",
    "\n",
    "text_matrix(jul_message,tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countvectorizer,TF-IDF,n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "arr = [\"Car was cleaned by Jack\",\n",
    "       \"Jack was cleaned by Car.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names \n",
      " ['by car', 'by jack', 'car was', 'cleaned by', 'jack was', 'was cleaned']\n",
      "Array \n",
      " [[0 1 1 1 0 1]\n",
      " [1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# If you want to take into account just term frequencies:\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# The ngram range specifies your ngram configuration.\n",
    "\n",
    "X = vectorizer.fit_transform(arr)\n",
    "\n",
    "# Testing the ngram generation:\n",
    "print(\"Feature Names \\n\",vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "print('Array \\n',X.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57615236 0.57615236 0.40993715 0.         0.40993715]\n",
      " [0.57615236 0.         0.         0.40993715 0.57615236 0.40993715]]\n"
     ]
    }
   ],
   "source": [
    "# And now testing TFIDF vectorizer:\n",
    "# You can still specify n-grams here.\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(arr)\n",
    "\n",
    "\n",
    "# Testing the TFIDF value + ngrams:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.40546511 1.40546511 1.         0.         1.        ]\n",
      " [1.40546511 0.         0.         1.         1.40546511 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing TFIDF vectorizer without normalization:\n",
    "# You can still specify n-grams here.\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2), norm=None)\n",
    "\n",
    "X = vectorizer.fit_transform(arr)\n",
    "\n",
    "# Testing TFIDF value before normalization:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
